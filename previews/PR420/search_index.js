var documenterSearchIndex = {"docs":
[{"location":"wavefront_ops/#Wavefront-Operations","page":"Wavefront Operations","title":"Wavefront Operations","text":"","category":"section"},{"location":"wavefront_ops/","page":"Wavefront Operations","title":"Wavefront Operations","text":"These intrinsics provide efficient operations across wavefronts.","category":"page"},{"location":"wavefront_ops/","page":"Wavefront Operations","title":"Wavefront Operations","text":"AMDGPU.wfred","category":"page"},{"location":"execution_control/#Execution-Control-and-Intrinsics","page":"Execution Control","title":"Execution Control and Intrinsics","text":"","category":"section"},{"location":"execution_control/","page":"Execution Control","title":"Execution Control","text":"GPU execution is similar to CPU execution in some ways, although there are many differences. AMD GPUs have Compute Units (CUs), which can be thought of like CPU cores. Those CUs have (on pre-Navi architectures) 64 \"shader processors\", which are essentially the same as CPU SIMD lanes. The lanes in a CU operate in lockstep just like CPU SIMD lanes, and have execution masks and various kinds of SIMD instructions available. CUs execute wavefronts, which are pieces of work split off from a single kernel launch. A single CU can run one out of many wavefronts (one is chosen by the CU scheduler each cycle), which allows for very efficient parallel and concurrent execution on the device. Each wavefront runs independently of the other wavefronts, only stopping to synchronize with other wavefronts or terminate when specified by the program.","category":"page"},{"location":"execution_control/","page":"Execution Control","title":"Execution Control","text":"We can control wavefront execution through a variety of intrinsics provided by ROCm. For example, the endpgm() intrinsic stops the current wavefront's execution, and is also automatically inserted by the compiler at the end of each kernel (except in certain unique cases).","category":"page"},{"location":"execution_control/","page":"Execution Control","title":"Execution Control","text":"signal_completion(x) signals the \"kernel doorbell\" with the value x, which is the signal checked by the CPU wait call to determine when the kernel has completed. This doorbell is set to 0 automatically by GPU hardware once the kernel is complete.","category":"page"},{"location":"execution_control/","page":"Execution Control","title":"Execution Control","text":"sendmsg(x,y=0) and sendmsghalt(x,y=0) can be used to signal special conditions to the scheduler/hardware, such as making requests to stop wavefront generation, or halt all running wavefronts. Check the ISA manual for details!","category":"page"},{"location":"devices/#Devices/Agents","page":"Devices/Agents","title":"Devices/Agents","text":"","category":"section"},{"location":"devices/","page":"Devices/Agents","title":"Devices/Agents","text":"In AMDGPU, all GPU devices (also known as \"agents\" in HSA parlance) are auto-detected by the runtime, if they're supported. There are three classes of devices:","category":"page"},{"location":"devices/","page":"Devices/Agents","title":"Devices/Agents","text":"CPU\nGPU\nDSP","category":"page"},{"location":"devices/","page":"Devices/Agents","title":"Devices/Agents","text":"In AMDGPU, we only support compilation and execution on GPU devices, so we will henceforth limit discussion to those; however, you may see a kind Symbol available in the APIs of many device access functions, which defaults to :gpu, but could also be :cpu or :dsp.","category":"page"},{"location":"devices/","page":"Devices/Agents","title":"Devices/Agents","text":"AMDGPU maintains a global default device. The default device is relevant for all kernel and GPUArray operations; if one is not specified via @roc or an equivalent interface, then the default device is used for those operations, which affects compilation and kernel launch.","category":"page"},{"location":"devices/","page":"Devices/Agents","title":"Devices/Agents","text":"note: Task-Local Storage\nSince AMDGPU.jl relies on Task-Local Storage, this means that default devices are default only within a given task. Other tasks migh have different default devices if the user switched them.","category":"page"},{"location":"devices/","page":"Devices/Agents","title":"Devices/Agents","text":"The default device is accessible via AMDGPU.device(). This function returns a ROCDevice, which is a handle that references the device. The list of available devices can be queried with AMDGPU.devices to get a list of all known and potentially usable devices.","category":"page"},{"location":"devices/","page":"Devices/Agents","title":"Devices/Agents","text":"If you have a ROCDevice object, you can also switch the default device via AMDGPU.device!. This will switch it only within the task it is called from.","category":"page"},{"location":"devices/","page":"Devices/Agents","title":"Devices/Agents","text":"To select default device for newly created tasks, use AMDGPU.default_device!.","category":"page"},{"location":"devices/","page":"Devices/Agents","title":"Devices/Agents","text":"Additionally, devices have an associated numeric ID. The default device ID can be queried with AMDGPU.default_device_id, which returns an Int. This value is bounded between 1 and length(AMDGPU.devices()), and device 1 is the default device when AMDGPU is first loaded. The ID of the device associated with the current task can be queried with AMDGPU.device_id and changed with AMDGPU.device_id!.","category":"page"},{"location":"devices/","page":"Devices/Agents","title":"Devices/Agents","text":"AMDGPU.devices\nAMDGPU.device\nAMDGPU.device!\nAMDGPU.default_device\nAMDGPU.default_device!\nAMDGPU.device_id\nAMDGPU.device_id!\nAMDGPU.default_device_id\nAMDGPU.default_device_id!","category":"page"},{"location":"devices/#AMDGPU.devices","page":"Devices/Agents","title":"AMDGPU.devices","text":"devices(kind::Symbol = :gpu)\n\nGet list of all devices of the given kind. kind can be :cpu, :gpu or :dsp, although AMDGPU.jl supports execution only on :gpu devices.\n\n\n\n\n\n","category":"function"},{"location":"devices/#AMDGPU.device","page":"Devices/Agents","title":"AMDGPU.device","text":"device()::ROCDevice\n\nGet currently active device. This device is used when launching kernels via @roc.\n\n\n\n\n\ndevice(A::ROCArray) -> ROCDevice\n\nReturn the device associated with the array A.\n\n\n\n\n\n","category":"function"},{"location":"devices/#AMDGPU.device!","page":"Devices/Agents","title":"AMDGPU.device!","text":"device!(device::ROCDevice)\n\nSwitch current device being used. This switches only for a task inside which it is called.\n\nnote: Note\nTo select default device that will be used when creating new tasks, refer to default_device! for that.\n\n\n\n\n\n","category":"function"},{"location":"devices/#AMDGPU.default_device","page":"Devices/Agents","title":"AMDGPU.default_device","text":"default_device()::ROCDevice\n\nDefault device which will be used by default in tasks. Meaning when a task is created, it selects this device as default.\n\nAll subsequent uses rely on device() for device selection.\n\n\n\n\n\n","category":"function"},{"location":"devices/#AMDGPU.default_device!","page":"Devices/Agents","title":"AMDGPU.default_device!","text":"default_device!(device::ROCDevice)\n\nSet default device that will be used when creating new tasks.\n\nnote: Note\nThis does not change current device being used. Refer to device! for that.\n\n\n\n\n\n","category":"function"},{"location":"devices/#AMDGPU.device_id","page":"Devices/Agents","title":"AMDGPU.device_id","text":"device_id(device::ROCDevice, kind::Symbol=:gpu) -> Int\n\nReturns the numerical device ID for device. See default_device_id for details on the numbering semantics.\n\n\n\n\n\n","category":"function"},{"location":"devices/#AMDGPU.device_id!","page":"Devices/Agents","title":"AMDGPU.device_id!","text":"device_id!(idx::Integer, kind::Symbol=:gpu)\n\nSets the current device to AMDGPU.devices(kind)[idx]. See device_id for details on the numbering semantics.\n\n\n\n\n\n","category":"function"},{"location":"devices/#AMDGPU.default_device_id","page":"Devices/Agents","title":"AMDGPU.default_device_id","text":"default_device_id(kind::Symbol=:gpu) -> Int\n\nReturns the numeric ID of the current default device, which is in the range of 1:length(AMDGPU.devices(kind)). This number should be stable for all processes on the same node, so long as any device filtering is consistently applied (such as ROCR_VISIBLE_DEVICES). The default_device_id! function accepts the same numeric ID that is produced by this function.\n\n\n\n\n\n","category":"function"},{"location":"devices/#AMDGPU.default_device_id!","page":"Devices/Agents","title":"AMDGPU.default_device_id!","text":"default_device_id!(idx::Integer, kind::Symbol=:gpu)\n\nSets the default device to AMDGPU.devices(kind)[idx]. See default_device_id for details on the numbering semantics.\n\n\n\n\n\n","category":"function"},{"location":"logging/#Runtime-and-Compiler-Logging","page":"Logging","title":"Runtime and Compiler Logging","text":"","category":"section"},{"location":"logging/","page":"Logging","title":"Logging","text":"AMDGPU.jl has a built-in logging system integrated into various runtime and compiler operations, which is provided by TimespanLogging.jl. Operations such as compilation and linking, signal and buffer allocation/freeing, kernel launch, etc. are instrumented with logging statements, allowing the user to record the start and end of operations.","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"While disabled by default, logging can be enabled by first running AMDGPU.Runtime.enable_logging!() to globally enable logging, after which Julia must be restarted for the changes to take effect.","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"Once logging is globally enabled, AMDGPU.Runtime.start_logging() causes new log events to be saved, while AMDGPU.Runtime.stop_logging() causes new log events to be discarded. Log events can be collected with AMDGPU.Runtime.fetch_logs!(). A more convenient option is AMDGPU.Runtime.log_and_fetch!(f), which can be used to easily log operations within a region of code:","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"logs = AMDGPU.Runtime.log_and_fetch!() do\n    A = AMDGPU.ones(3, 4)\n    B = copy(A)\n    fill!(B, 1f0)\n    C = Array(B)\nend\n@show logs[1]","category":"page"},{"location":"api/#AMDGPU-API-Reference","page":"API Reference","title":"AMDGPU API Reference","text":"","category":"section"},{"location":"api/#Kernel-launching","page":"API Reference","title":"Kernel launching","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"@roc\nAMDGPU.AbstractKernel\nAMDGPU.HostKernel\nAMDGPU.rocfunction","category":"page"},{"location":"api/#AMDGPU.@roc","page":"API Reference","title":"AMDGPU.@roc","text":"@roc [kwargs...] func(args...)\n\nHigh-level interface for executing code on a GPU. The @roc macro should prefix a call, with func a callable function or object that should return nothing. It will be compiled to a GCN function via rocfunction upon first use, and to a certain extent arguments will be converted and managed automatically using rocconvert. Finally, a call to roccall is performed, scheduling a kernel launch on the specified (or default) HSA queue.\n\nSeveral keyword arguments are supported that influence the behavior of @roc.\n\nKeyword arguments that control general @roc behavior:\n\ndynamic::Bool = false: Use dynamic parallelism to launch as a device-side kernel\nlaunch::Bool = true: Whether to launch the kernel\nwait::Bool = true: Whether to wait on all arguments' dependencies\nmark::Bool = true: Whether to mark this kernel as a dependency for all arguments\n\nKeyword arguments that affect various parts of @roc:\n\ndevice::ROCDevice = AMDGPU.default_device(): The device to compile code for, and launch the kernel on.\nqueue::ROCQueue = AMDGPU.queue(device): Which queue to associate the kernel (and its completion signal) with. May also be specified as stream for compatibility with CUDA.jl.\n\nKeyword arguments that control kernel compilation via rocfunction and dynamic_rocfunction:\n\nname::Union{String,Nothing} = nothing: If not nothing, the name to use for the generated kernel.\nglobal_hooks::NamedTuple = (;): The set of global compiler hooks to use to initialize memory accessed by the kernel. See AMDGPU.Compiler.default_global_hooks for an example of how to implement these.\n\nKeyword arguments that control signal creation via AMDGPU.create_event:\n\nsignal::ROCSignal = ROCSignal(): The underlying signal object to associate the high-level ROCKernelSignal with.\nsoft::Bool = true: Whether to use the \"soft\" busy-poll waiter algorithm. If false, uses HSA's built-in blocking wait.\nminlat::Float64 = 0.000001: The minimum latency allowed on the first wait cycle. Specifically, if the kernel completes in less than this amount of time, then the observed latency from kernel launch to return from wait is this value, in seconds.\ntimeout::Union{Float64, Nothing} = nothing: How long to wait for the signal to complete before throwing an AMDGPU.Runtime.SignalTimeoutException, in seconds. If nothing, then timeouts are disabled and the wait call may hang forever if the kernel never completes.\n\nKeyword arguments that control kernel creation via AMDGPU.create_kernel:\n\nlocalmem::Int = 0: The amount of dynamic local memory to allocate for the kernel. This value is separate from the amount of static local memory required by the kernel (as reported by the compiler).\n\nKeyword arguments that control kernel launch via AMDGPU.HostKernel and AMDGPU.DeviceKernel:\n\ngroupsize::Union{Tuple,Integer} = 1: The size of the groups to execute over the grid. If an Integer or Tuple{<:Integer}, only activate the X dimension of the group. If Tuple{<:Integer,<:Integer}, activate the X and Y dimensions of the group. If Tuple{<:Integer,<:Integer,<:Integer}, activate the X, Y, and Z dimensions of the group. All sizes must be greater than 0.\ngridsize::Union{Tuple,Integer} = 1: The size of the grid to execute the kernel over. If an Integer or Tuple{<:Integer}, only activate the X dimension of the grid. If Tuple{<:Integer,<:Integer}, activate the X and Y dimensions of the grid. If Tuple{<:Integer,<:Integer,<:Integer}, activate the X, Y, and Z dimensions of the grid. All sizes must be greater than 0.\nthreads::Union{Tuple,Integer} - Alias for groupsize, for compatibility with CUDA.jl.\nblocks::Union{Tuple,Integer} - How many groups to execute across the grid. Potentially a more convenient way to specify groupsize, and intended for compatibility with CUDA.jl.\n\nThe underlying operations (argument conversion, kernel compilation, kernel call) can be performed explicitly when more control is needed, e.g. to reflect on the resource usage of a kernel to determine the launch configuration. A host-side kernel launch is done as follows:\n\nargs = ...\nGC.@preserve args begin\n    kernel_f = rocconvert(f)\n    kernel_args = rocconvert.(args)\n    kernel_tt = Tuple{Core.Typeof.(kernel_args)...}\n    kernel = rocfunction(kernel_f, kernel_tt; compilation_kwargs)\n    kernel(kernel_args...; launch_kwargs)\nend\n\nA device-side launch, aka. dynamic parallelism, is similar but more restricted:\n\nargs = ...\n# GC.@preserve is not supported\n# we're on the device already, so no need to rocconvert\nkernel_tt = Tuple{Core.Typeof(args[1]), ...}    # this needs to be fully inferred!\nkernel = dynamic_rocfunction(f, kernel_tt)       # no compiler kwargs supported\nkernel(args...; launch_kwargs)\n\n\n\n\n\n","category":"macro"},{"location":"api/#AMDGPU.Compiler.rocfunction","page":"API Reference","title":"AMDGPU.Compiler.rocfunction","text":"rocfunction(f, tt=Tuple{}; kwargs...)\n\nLow-level interface to compile a function invocation for the currently-active GPU, returning a callable kernel object. For a higher-level interface, use @roc.\n\nThe following keyword arguments are supported:\n\nname: overrides the name that the kernel will have in the generated code\ndevice: chooses which device to compile the kernel for\nglobal_hooks: specifies maps from global variable name to initializer hook\n\nThe output of this function is automatically cached, i.e. you can simply call rocfunction in a hot path without degrading performance. New code will be generated automatically, when function definitions change, or when different types or keyword arguments are provided.\n\n\n\n\n\n","category":"function"},{"location":"api/#Device-code-API","page":"API Reference","title":"Device code API","text":"","category":"section"},{"location":"api/#Thread-indexing","page":"API Reference","title":"Thread indexing","text":"","category":"section"},{"location":"api/#HSA-nomenclature","page":"API Reference","title":"HSA nomenclature","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"AMDGPU.workitemIdx\nAMDGPU.workgroupIdx\nAMDGPU.workgroupDim\nAMDGPU.gridItemDim\nAMDGPU.gridGroupDim","category":"page"},{"location":"api/#AMDGPU.Device.workitemIdx","page":"API Reference","title":"AMDGPU.Device.workitemIdx","text":"workitemIdx()::ROCDim3\n\nReturns the work item index within the work group. See also: threadIdx\n\n\n\n\n\n","category":"function"},{"location":"api/#AMDGPU.Device.workgroupIdx","page":"API Reference","title":"AMDGPU.Device.workgroupIdx","text":"workgroupIdx()::ROCDim3\n\nReturns the work group index. See also: blockIdx\n\n\n\n\n\n","category":"function"},{"location":"api/#AMDGPU.Device.workgroupDim","page":"API Reference","title":"AMDGPU.Device.workgroupDim","text":"workgroupDim()::ROCDim3\n\nReturns the size of each workgroup in workitems. See also: blockDim\n\n\n\n\n\n","category":"function"},{"location":"api/#AMDGPU.Device.gridItemDim","page":"API Reference","title":"AMDGPU.Device.gridItemDim","text":"gridItemDim()::ROCDim3\n\nReturns the size of the grid in workitems. This behaviour is different from CUDA where gridDim gives the size of the grid in blocks.\n\n\n\n\n\n","category":"function"},{"location":"api/#AMDGPU.Device.gridGroupDim","page":"API Reference","title":"AMDGPU.Device.gridGroupDim","text":"gridGroupDim()::ROCDim3\n\nReturns the size of the grid in workgroups. This is equivalent to CUDA's gridDim.\n\n\n\n\n\n","category":"function"},{"location":"api/#CUDA-nomenclature","page":"API Reference","title":"CUDA nomenclature","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Use these functions for compatibility with CUDA.jl.","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"AMDGPU.Device.threadIdx\nAMDGPU.Device.blockIdx\nAMDGPU.Device.blockDim","category":"page"},{"location":"api/#AMDGPU.Device.threadIdx","page":"API Reference","title":"AMDGPU.Device.threadIdx","text":"threadIdx()::ROCDim3\n\nReturns the thread index within the block. See also: workitemIdx\n\n\n\n\n\n","category":"function"},{"location":"api/#AMDGPU.Device.blockIdx","page":"API Reference","title":"AMDGPU.Device.blockIdx","text":"blockIdx()::ROCDim3\n\nReturns the block index within the grid. See also: workgroupIdx\n\n\n\n\n\n","category":"function"},{"location":"api/#AMDGPU.Device.blockDim","page":"API Reference","title":"AMDGPU.Device.blockDim","text":"blockDim()::ROCDim3\n\nReturns the dimensions of the block. See also: workgroupDim\n\n\n\n\n\n","category":"function"},{"location":"api/#Synchronization","page":"API Reference","title":"Synchronization","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"AMDGPU.sync_workgroup","category":"page"},{"location":"api/#AMDGPU.Device.sync_workgroup","page":"API Reference","title":"AMDGPU.Device.sync_workgroup","text":"sync_workgroup()\n\nWaits until all wavefronts in a workgroup have reached this call.\n\n\n\n\n\n","category":"function"},{"location":"api/#Global-Variables","page":"API Reference","title":"Global Variables","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"AMDGPU.Device.get_global_pointer","category":"page"},{"location":"queues_signals/#Queues","page":"Queues and Signals","title":"Queues","text":"","category":"section"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"Similar to CUDA streams, ROCm has the concept of queues, which are buffers used to instruct the GPU hardware which kernels to launch. ROCm queues are synchronous, like CUDA streams.","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"Each device has a default queue associated, which is accessible with AMDGPU.queue.","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"To specify which queue to launch a kernel on:","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"Using AMDGPU.queue!, which will execute given function and reset   to the original queue after completion:","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"q = AMDGPU.ROCQueue()\nx = AMDGPU.queue!(() -> AMDGPU.ones(Float32, 16), q)","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"Using queue argument to @roc macro:","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"q = AMDGPU.ROCQueue()\n@roc queue=q kernel(...)","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"Queues also have an inherent priority, which allows control of kernel submission latency and on-device scheduling preference with respect to kernels submitted on other queues. There are three priorities: normal (the default), low, and high priority.","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"Priority of the default queue can be set with AMDGPU.priority!. Alternatively, it can be set at queue creation time:","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"low_prio_queue = ROCQueue(; priority=:low)\nhigh_prio_queue = ROCQueue(; priority=:high)\nnormal_prio_queue = ROCQueue(; priority=:normal) # or just omit \"priority\"","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"To get kernels which are currently executing on a given queue, use AMDGPU.active_kernels. It will return a Vector{ROCKernelSignal}, which can be inspected to determine how many (and which) kernels are executing.","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"If a kernel gets \"stuck\" and locks up the GPU (noticeable with 100% GPU usage in rocm-smi) you can kill it and all other kernels associated with the queue it is running on with AMDGPU.Runtime.kill_queue!(queue). This can be \"safely\" done to the default queue (obtained via AMDGPU.queue), since default queues are recreated as-needed.","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"AMDGPU.queue\nAMDGPU.queue!\nAMDGPU.priority!\nAMDGPU.active_kernels\nAMDGPU.ROCQueue\nAMDGPU.Runtime.set_queue_pool_size!\nAMDGPU.Runtime.kill_queue!","category":"page"},{"location":"queues_signals/#AMDGPU.queue","page":"Queues and Signals","title":"AMDGPU.queue","text":"queue()::ROCQueue\n\nGet task-local default queue for the currently active device.\n\n\n\n\n\n","category":"function"},{"location":"queues_signals/#AMDGPU.queue!","page":"Queues and Signals","title":"AMDGPU.queue!","text":"queue!(f::Base.Callable, queue::ROCQueue)\n\nChange default queue, execute given function f and revert back to the original queue.\n\nReturns\n\nReturn value of the function f.\n\n\n\n\n\n","category":"function"},{"location":"queues_signals/#AMDGPU.priority!","page":"Queues and Signals","title":"AMDGPU.priority!","text":"priority!(priority::Symbol)\n\nChange the priority of the default queue. Accepted values are :normal (the default), :low and :high.\n\n\n\n\n\npriority!(f::Base.Callable, priority::Symbol)\n\nChnage the priority of default queue, execute f and revert to the original priority. Accepted values are :normal (the default), :low and :high.\n\nReturns\n\nReturn value of the function f.\n\n\n\n\n\n","category":"function"},{"location":"queues_signals/#AMDGPU.active_kernels","page":"Queues and Signals","title":"AMDGPU.active_kernels","text":"active_kernels(queue::ROCQueue = queue()) -> Vector{ROCKernelSignal}\n\nReturns the set of actively-executing kernels on queue.\n\n\n\n\n\n","category":"function"},{"location":"queues_signals/#AMDGPU.Runtime.ROCQueue","page":"Queues and Signals","title":"AMDGPU.Runtime.ROCQueue","text":"ROCQueue(; priority::Symbol=:normal, pooled::Bool=false)\n\nCreate an HSA queue on the currently active device.\n\nnote: Note\nUsers are encouraged to use this method, instead of manually providing device since this one correctly handles device changes.\n\n\n\n\n\nROCQueue(device::ROCDevice; priority::Symbol=:normal, pooled::Bool=false)\n\nCreate an HSA queue which will be used to instruct GPU hardware which kernels to launch.\n\nEach queue, spawns an error monitoring thread that's responsible for actually waiting on kernels and performing a cleanup after kernel finished its execution.\n\nnote: Oversubscribed Command Queues in GPUs\nBe careful, with the number of HSA queues in use. When the number of allocated HSA queues is greater than the number of hardware queues, the GPU wastes significant time rotating between all allocated queues in search of ready tasks.\n\nArguments:\n\ndevice::ROCDevice: Device on which to create queue.\npriority::Symbol: Queue's priority. Can be :normal, :low, :high.\npooled::Bool: Whether to use pool when creating queues.   When true, queues are drawn from it on creation   and returned to pool instead of destroyed.\n\n\n\n\n\n","category":"type"},{"location":"queues_signals/#AMDGPU.Runtime.set_queue_pool_size!","page":"Queues and Signals","title":"AMDGPU.Runtime.set_queue_pool_size!","text":"set_queue_pool_size!(nums::NTuple{3, Int})\n\nSet HSA queue pool max size for each priority. Restart Julia session for the changes to take effect.\n\nArguments:\n\nnums::NTuple{3, Int}: Maximum number of queues for :normal,   :low and :high priority.   Providing 0 for specific priority, disables pool for it.\n\n\n\n\n\n","category":"function"},{"location":"queues_signals/#AMDGPU.Runtime.kill_queue!","page":"Queues and Signals","title":"AMDGPU.Runtime.kill_queue!","text":"kill_queue!(queue::ROCQueue)\n\nKill queue and propagate queue error to all waiter signals in case if there is one.\n\nIf queue is in the pool, it will be removed from it.\n\nnote: Note\nNo need to manually call this function during regular use, it will be called automatically from ROCQueue finalizer.\n\n\n\n\n\n","category":"function"},{"location":"queues_signals/#Signals","page":"Queues and Signals","title":"Signals","text":"","category":"section"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"Unlike CUDA, ROCm kernels are tracked by an associated signal, which is created and returned by @roc, and is waited on to track kernel completion. Signals may also be used for manual synchronization (since they work for CPUs and GPUs equally well). CPU usage is done with the HSA.signal_* functions, and GPU usage is done with the device_signal_* and hostcall_device_signal_* functions. For most signalling needs, consider using a hostcall instead.","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"If custom signal handling is desired, signals can be manually constructed and passed to @roc:","category":"page"},{"location":"queues_signals/","page":"Queues and Signals","title":"Queues and Signals","text":"# A kernel which waits on all signals in `sigs`\nfunction multi_wait(sigs)\n    for i in 1:length(sigs)\n        AMDGPU.Device.hostcall_device_signal_wait(sigs[i], 0)\n    end\n    nothing\nend\n\n# Create a set of signals\nsigs = [ROCSignal() for i in 1:10]\n# Get the device-safe signal handles\n_sigs = ROCArray(map(sig->sig.signal, sigs))\n\n# Launch multi-waiter ahead of time; this will block on the device\nfinal_sig = @roc multi_wait(_sigs)\n\n# Associate kernels with signals\nfor sig in sigs\n    @roc signal=sig identity(nothing)\nend\n\n# Wait on the multi-waiter\nwait(final_sig)","category":"page"},{"location":"globals/#Global-Variables","page":"Global Variables","title":"Global Variables","text":"","category":"section"},{"location":"globals/","page":"Global Variables","title":"Global Variables","text":"Most programmers are familiar with the concept of a \"global variable\": a variable which is globally accessible to any function in the user's program. In Julia, programmers are told to avoid using global variables (also known as \"globals\") because of their tendency to introduce type instabilities. However, they're often useful for sharing data between functions in distinct areas of the user's program.","category":"page"},{"location":"globals/","page":"Global Variables","title":"Global Variables","text":"In the JuliaGPU ecosystem, globals in the Julia sense are not available unless their value is constant and inlinable into the function referencing them, as all GPU kernels must be statically compileable. However, a different sort of global variable is available which serves a very similar purpose. This variant of global variable is statically typed and sized, and is accessible from: all kernels with the same function signature (e.g. mykernel(a::Int32, b::Float64)), the CPU host, and other devices and kernels when accessed by pointer.","category":"page"},{"location":"globals/","page":"Global Variables","title":"Global Variables","text":"Global variables can be created within kernels with the AMDGPU.Device.get_global_pointer function, which both declares the global variable, and returns a pointer to it (specifically a Core.LLVMPtr). Once a kernel which declares a global is compiled for GPU execution (either by @roc or rocfunction), the global is allocated memory and made available to the kernel (during the linking stage). Globals are unique by name, and so you shouldn't attempt to call get_global_pointer with the same name but a different type; if you do, undefined behavior will result. Like regular pointers in Julia, you can use functions like Base.unsafe_load and Base.unsafe_store! to read from and write to the global variable, respectively.","category":"page"},{"location":"globals/","page":"Global Variables","title":"Global Variables","text":"As a concrete example of global variable usage, let's define a kernel which creates a global and uses its value to increment the indices of an array:","category":"page"},{"location":"globals/","page":"Global Variables","title":"Global Variables","text":"function my_kernel(A)\n    idx = AMDGPU.Device.workitemIdx().x\n    ptr = AMDGPU.Device.get_global_pointer(Val(:myglobal), Float32)\n    A[idx] += Base.unsafe_load(ptr)\n    nothing\nend","category":"page"},{"location":"globals/","page":"Global Variables","title":"Global Variables","text":"In order to access and modify this global before the kernel is launched, we can specify a hook function to @roc which will be passed the global pointer as an argument:","category":"page"},{"location":"globals/","page":"Global Variables","title":"Global Variables","text":"function myglobal_hook(gbl, mod, dev)\n    gbl_ptr = Base.unsafe_convert(Ptr{Float32}, gbl.ptr)\n    Base.unsafe_store!(gbl_ptr, 42f0)\nend\nRA = ROCArray(ones(Float32, 4))\nwait(@roc groupsize=4 global_hooks=(myglobal=myglobal_hook,) my_kernel(RA))","category":"page"},{"location":"globals/","page":"Global Variables","title":"Global Variables","text":"In the above function, gbl_ptr is a pointer (specifically a Ptr{Float32}) to the memory that represents the global variable myglobal. We can't guarantee the initial value of an uninitialized global variable, so we need to write a value to that global variable (in this case 42::Float32).","category":"page"},{"location":"globals/","page":"Global Variables","title":"Global Variables","text":"We can then read the values of RA and see that it's what we expect:","category":"page"},{"location":"globals/","page":"Global Variables","title":"Global Variables","text":"julia> A = Array(RA)\n4-element ROCArray{Float32,1}:\n 43.0\n 43.0\n 43.0\n 43.0","category":"page"},{"location":"kernel_launch/#Kernel-Launch","page":"Kernel Launch","title":"Kernel Launch","text":"","category":"section"},{"location":"kernel_launch/#Launch-Configuration","page":"Kernel Launch","title":"Launch Configuration","text":"","category":"section"},{"location":"kernel_launch/","page":"Kernel Launch","title":"Kernel Launch","text":"While an almost arbitrarily large number of workitems can be executed per kernel launch, the hardware can only support executing a limited number of wavefronts at one time. To alleviate this, the compiler calculates the \"occupancy\" of each compiled kernel (which is the number of wavefronts that can be simultaneously executing on the GPU), and passes this information to the hardware; the hardware then launches a limited number of wavefronts at once, based on the kernel's \"occupancy\" values. The rest of the wavefronts are not launched until hardware resources become available, which means that a kernel with better occupancy will see more of its wavefronts executing simultaneously (which often leads to better performance). Suffice to say, it's important to know the occupancy of kernels if you want the best performance.","category":"page"},{"location":"kernel_launch/","page":"Kernel Launch","title":"Kernel Launch","text":"Like CUDA.jl, AMDGPU.jl has the ability to calculate kernel occupancy, with the launch_configuration function:","category":"page"},{"location":"kernel_launch/","page":"Kernel Launch","title":"Kernel Launch","text":"kernel = @roc launch=false mykernel(args...)\noccupancy = AMDGPU.launch_configuration(kernel)\n@show occupancy.groupsize","category":"page"},{"location":"kernel_launch/","page":"Kernel Launch","title":"Kernel Launch","text":"Specifically, launch_configuration calculates the occupancy of mykernel(args...), and then calculates an optimal groupsize based on the occupancy. This value can then be used to select the groupsize for the kernel:","category":"page"},{"location":"kernel_launch/","page":"Kernel Launch","title":"Kernel Launch","text":"wait(@roc groupsize=occupancy.groupsize mykernel(args...)","category":"page"},{"location":"kernel_launch/","page":"Kernel Launch","title":"Kernel Launch","text":"While it works, it's also pretty verbose. Conveniently, there's also a mechanism to do all of the above automatically within @roc:","category":"page"},{"location":"kernel_launch/","page":"Kernel Launch","title":"Kernel Launch","text":"wait(@roc groupsize=:auto mykernel(args...))","category":"page"},{"location":"kernel_launch/","page":"Kernel Launch","title":"Kernel Launch","text":"The above is safe to do in a hot path, as the occupancy is cached on a per-kernel basis.","category":"page"},{"location":"kernel_launch/","page":"Kernel Launch","title":"Kernel Launch","text":"There are also various other details available from the occupancy calculation, such as SGPR, VGPR, and LDS usage, wavefront size, etc.:","category":"page"},{"location":"kernel_launch/","page":"Kernel Launch","title":"Kernel Launch","text":"kernel = @roc launch=false mykernel(args...)\n@show AMDGPU.Compiler.calculate_occupancy(kernel.fun, AMDGPU.default_device())","category":"page"},{"location":"quickstart/#Quick-Start","page":"Quick Start","title":"Quick Start","text":"","category":"section"},{"location":"quickstart/#Installation","page":"Quick Start","title":"Installation","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"See JLL usage for info about ROCm stack installation. Simply add the AMDGPU.jl package to your Julia environment:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"using Pkg\nPkg.add(\"AMDGPU\")","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"You can then load the AMDGPU package and run the unit tests:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"using AMDGPU\nusing Pkg\nPkg.test(\"AMDGPU\")","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"warning: Warning\nIf you get an error message along the lines of GLIB_CXX_... not found, it's possible that the C++ runtime used to build the ROCm stack and the one used by Julia are different. If you built the ROCm stack yourself this is very likely the case since Julia normally ships with its own C++ runtime. For more information, check out this GitHub issue.A quick fix is to use the LD_PRELOAD environment variable to make Julia use the system C++ runtime library, for example:LD_PRELOAD=/usr/lib/libstdc++.so juliaAlternatively, you can build Julia from source as described here.You can quickly debug this issue by starting Julia and trying to load a ROCm library:using Libdl\nLibdl.dlopen(\"/opt/rocm/hsa/lib/libhsa-runtime64.so.1\")","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"warning: Warning\nIf during the build process you get an error message along the lines of hipErrorNoBinaryForGpu: Coudn't find binary for current devices! and you already have ROCm installed locally then you should set the environment variable JULIA_AMDGPU_DISABLE_ARTIFACTS=1 and reload AMDGPU.jl.","category":"page"},{"location":"quickstart/#Running-a-simple-kernel","page":"Quick Start","title":"Running a simple kernel","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"As a simple test, we will try to add two random vectors and make sure that the results from the CPU and the GPU are indeed the same.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"We can start by first performing this simple calculation on the CPU:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"N = 32\na = rand(Float64, N)\nb = rand(Float64, N)\nc_cpu = a + b","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"To do the same computation on the GPU, we first need to copy the two input arrays a and b to the device. Toward that end, we will use the ROCArray type to represent our GPU arrays. We can create the two arrays by passing the host data to the constructor as follows:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"using AMDGPU\na_d = ROCArray(a)\nb_d = ROCArray(b)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"We need to create one additional array c_d to store the results:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"c_d = similar(a_d)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"In this example, the postfix _d distinguishes a device memory object from its host memory counterpart. This convention is completely arbitrary and you may name your device-side variables whatever you like; they are regular Julia variables.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Next, we will define the GPU kernel that does the actual computation:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"function vadd!(c, a, b)\n    i = workitemIdx().x\n    c[i] = a[i] + b[i]\n    return\nend","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"This simple kernel starts by getting the current thread ID using workitemIdx and then performs the addition of the elements from a and b, storing the result in c.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Like OpenCL, AMDGPU has the concept of \"workitems\", \"workgroups\", and the \"grid\". A workitem is a single thread of execution, capable of performing arithmentic operations. Workitems are grouped into \"wavefronts\" (\"warps\" in CUDA) which share the same compute unit, and execute the same instructions simulatenously. The workgroup is a logical unit of compute supported by hardware which comprises multiple wavefronts, which shares resources (specifically local memory) and can be efficiently synchronized. A workgroup may be executed by one or multiple hardware compute units, making it often the only dimension of importance for smaller kernel launches.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"The grid is the domain over which the entire kernel executes over. The index of a single workitem can be uniquely identified by its grid index (computed linearly as (workgroupDim().x * (workgroupIdx().x - 1)) + workitemIdx().x when only a single dimension is used). The grid will be split into multiple workgroups by hardware automatically, and the kernel does not complete until all workgroups complete.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Notice how we explicitly specify that this function does not return a value by adding the return statement. This is necessary for all GPU kernels and we can enforce it by adding a return, return nothing, or even nothing at the end of the kernel. If this statement is omitted, Julia will attempt to return the value of the last evaluated expression, in this case a Float64, which will cause a compilation failure as kernels cannot return values.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"The easiest way to launch a GPU kernel is with the @roc macro, specifying that we want a single work group with N work items and calling it like an ordinary function:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"@roc groupsize=N vadd!(c_d, a_d, b_d)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Keep in mind that kernel launches are asynchronous, meaning that you need to do some kind of synchronization before you use the result. For instance, you can call wait() on the returned HSA signal value:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"wait(@roc groupsize=N vadd!(c_d, a_d, b_d))","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"warning: Naming conventions\nThroughout this example we use terms like \"work group\" and \"work item\". These terms are used by the Khronos consortium and their APIs including OpenCL and Vulkan, as well as the HSA foundation.NVIDIA, on the other hand, uses some different terms in their CUDA API, which might be confusing to some users porting their kernels from CUDA to AMDGPU. As a quick summary, here is a mapping of the most common terms:AMDGPU CUDA\nworkitemIdx threadIdx\nworkgroupIdx blockIdx\nworkgroupDim blockDim\ngridItemDim No equivalent\ngridGroupDim gridDim\ngroupsize threads\ngridsize blocks * threads\nqueue streamFor compatibilty reasons, the symbols in the CUDA column (except for gridItemDim) are also supported by AMDGPU.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Finally, we can make sure that the results match, by first copying the data to the host and then comparing it with the CPU results:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"c = Array(c_d)\n\nusing Test\n@test isapprox(c, c_cpu)","category":"page"},{"location":"memory/#Memory-Allocation-and-Intrinsics","page":"Memory","title":"Memory Allocation and Intrinsics","text":"","category":"section"},{"location":"memory/#Memory-Varieties","page":"Memory","title":"Memory Varieties","text":"","category":"section"},{"location":"memory/","page":"Memory","title":"Memory","text":"GPUs contain various kinds of memory, just like CPUs:","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"Global: Globally accessible by all CUs on a GPU, and possibly accessible from outside of the GPU (by the CPU host, by other GPUs, by PCIe devices, etc.). Slowest form of memory.\nConstant: Same as global memory, but signals to the hardware that it can use special instructions to access and cache this memory. Can be changed between kernel invocations.\nRegion: Also known as Global Data Store (GDS), all wavefronts on a CU can access the same memory region from the same address. Faster than Global/Constant. Automatically allocated by the compiler/runtime, not user accessible.\nLocal: Also known as Local Data Store (LDS), all wavefronts in the same workgroup can access the same memory region from the same address. Faster than GDS.\nPrivate: Uses the hardware scratch space, and is private to each SIMD lane in a wavefront. Fastest form of traditional memory.","category":"page"},{"location":"memory/#Memory-Queries","page":"Memory","title":"Memory Queries","text":"","category":"section"},{"location":"memory/","page":"Memory","title":"Memory","text":"Most useable memory can be queried via AMDGPU's APIs. There are two sets of APIs for querying memory: the older \"Regions\" API, and the newer \"Memory Pools\" API. The Regions API is able to query all kinds of memory segments for each device:","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"for device in AMDGPU.devices()\n    foreach(println, AMDGPU.Runtime.regions(device)::Vector{AMDGPU.Runtime.ROCMemoryRegion})\nend","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"The Memory Pools API is currently only able to query Global and Group memory segments, but is more reliable (due to getting more development attention from AMD):","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"for device in AMDGPU.devices()\n    foreach(println, AMDGPU.Runtime.memory_pools(device)::Vector{AMDGPU.Runtime.ROCMemoryPool})\nend","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"Most of the details of each memory segment are available by printing the region or memory pool in question; they may also be accessed programmatically with the AMDGPU.Runtime.region_* and AMDGPU.Runtime.pool_* APIs.","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"These details are generally not important to the average user, and are handled automatically by AMDGPU when memory is allocated and freed.","category":"page"},{"location":"memory/#Memory-Allocation/Deallocation","page":"Memory","title":"Memory Allocation/Deallocation","text":"","category":"section"},{"location":"memory/","page":"Memory","title":"Memory","text":"Currently, we can explicitly allocate Global and Local memory from within kernels, and Global from outside of kernels. Global memory allocations are done with AMDGPU.Mem.alloc, like so:","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"buf = AMDGPU.Mem.alloc(device, bytes)\n# Or with the extended API if a region or memory pool is already selected:\nbuf = AMDGPU.Mem.alloc(device, pool, bytes)\nbuf = AMDGPU.Mem.alloc(device, region, bytes)","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"buf in this example is a Mem.Buffer, which contains a pointer to the allocated memory. The buffer can be converted to a pointer by doing Base.unsafe_convert(Ptr{Nothing}, buf), and may then be converted to the appropriate pointer type, and loaded from/stored to. By default, memory is allocated specifically on and for device, and is only accessible to that device unless transferred using the various functions in the Mem module. If memory should be globally accessible by the CPU and by all GPUs, the kwarg coherent=true may be passed, which utilizes \"finegrained\" memory instead. Memory should be freed once it's no longer in use with Mem.free(buf).","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"If allocations are done as Mem.alloc(device, bytes; coherent=false, slow_fallback=true), and the allocation is larger than supported for that memory region, then coherent memory will be automatically used (if possible) to service the allocation. This can be disabled with AMDGPU.Mem.enable_slow_allocation_fallback(false) and restarting Julia.","category":"page"},{"location":"memory/#Device-Side-Allocations","page":"Memory","title":"Device-Side Allocations","text":"","category":"section"},{"location":"memory/","page":"Memory","title":"Memory","text":"Global memory allocated by a kernel is automatically freed when the kernel completes, which is done in the wait call on the host. This behavior can be disabled by passing cleanup=false to wait.","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"Global memory may also be allocated and freed dynamically from kernels by calling AMDGPU.Device.malloc(::Csize_t)::Ptr{Cvoid} and AMDGPU.Device.free(::Ptr{Cvoid}).  This memory allocation/deallocation uses hostcalls to operate, and so is relatively slow, but is also very useful. Currently, memory allocated with AMDGPU.Device.malloc is coherent by default. Calls to malloc and free are performed once per workgroup, so ensure that enough memory has been allocated to feed the lanes that will be accessing it.","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"As an example, here's how an array could be allocated on-device to store temporary results:","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"function kernel(C, A, B)\n    # Allocate memory dynamically and get a pointer to it\n    Ctmp_ptr = AMDGPU.Device.malloc(Csize_t(sizeof(Float64)*length(C)))\n    # Turn it (a pointer to Float64 elements in Global memory) into a device-side array\n    Ctmp = ROCDeviceArray(length(C), reinterpret(Core.LLVMPtr{Float64,1}, Ctmp_ptr))\n    # Use it\n    idx = AMDGPU.workitemIdx().x\n    Ctmp[idx] = A[idx] + B[idx] + C[1]\n    AMDGPU.Device.sync_workgroup()\n    C[idx] = Ctmp[idx]\n    # Make sure to free it\n    AMDGPU.Device.free(Ctmp_ptr)\n    nothing\nend\nRA = AMDGPU.rand(4)\nRB = AMDGPU.rand(4)\nRC = AMDGPU.rand(4)\nRC_elem = Array(RC)[1]\nwait(@roc groupsize=4 kernel(RC, RA, RB))\n@assert Array(RC)  Array(RA) .+ Array(RB) .+ RC_elem","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"Local memory may be allocated within a kernel by calling either @ROCStaticLocalArray(T, dims) or @ROCDynamicLocalArray(T, dims) - use the former if dims is passed as a constant value, and otherwise use the latter. Local memory does not need to be freed, as it is automatically freed by the hardware.","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"If @ROCDynamicLocalArray is used, then local memory is dynamically allocated at kernel execution time; the localmem option to @roc must be set appropriately to ensure that enough local memory is allocated by the hardware. It is allocated in addition to the local memory that is statically allocated by the kernel.","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"function kernel(C, A, B)\n    # Allocate local memory dynamically\n    Ctmp = @ROCDynamicLocalArray(Float64, length(C))\n    # Or, allocate local memory statically if the size is known ahead-of-time\n    # Ctmp = @ROCStaticLocalArray(Float64, 8) # if we want 8 elements\n\n    # Use it\n    idx = AMDGPU.workitemIdx().x\n    Ctmp[idx] = A[idx] + B[idx] + C[1]\n    AMDGPU.Device.sync_workgroup()\n    C[idx] = Ctmp[idx]\n    nothing\nend\n# ...\n# Note: The `localmem` option isn't necessary if `@ROCStaticLocalArray` is used\nwait(@roc groupsize=4 localmem=sizeof(Float64)*length(RC) kernel(RC, RA, RB))","category":"page"},{"location":"memory/","page":"Memory","title":"Memory","text":"Note that like CUDA's shared memory, AMDGPU's local memory is zero-initialized automatically. If this behavior is unnecessary (and undesired for performance reasons), zero-initialization can be disabled with @ROCDynamicLocalArray(T, dims, false) or @ROCStaticLocalArray(T, dims, false) (the last argument is zeroinit).","category":"page"},{"location":"memory/#Memory-Modification-Intrinsics","page":"Memory","title":"Memory Modification Intrinsics","text":"","category":"section"},{"location":"memory/","page":"Memory","title":"Memory","text":"Like C, AMDGPU.jl provides the memset! and memcpy! intrinsics, which are useful for setting a memory region to a value, or copying one region to another, respectively. Check test/device/memory.jl for examples of their usage.","category":"page"},{"location":"printing/#Printing","page":"Printing","title":"Printing","text":"","category":"section"},{"location":"printing/","page":"Printing","title":"Printing","text":"Writing GPU kernels can be a difficult endeavor, owing to the fact that the LLVM GPU backends turn serial code into parallel code automatically. Recognizing this, every good GPU programming interface allows the user's GPU kernels to print output to a buffer, which will be passed to the host for display. With the ability to interpolate variables, this functionality serves as the \"printf of GPUs\". Quite literally, the primary tool for this is @rocprintf. Here's a simple example of printing the current workgroup index:","category":"page"},{"location":"printing/","page":"Printing","title":"Printing","text":"kernel(x) = @rocprintf \"Workgroup index: %d\\n\" workgroupIdx().x","category":"page"},{"location":"printing/","page":"Printing","title":"Printing","text":"The above kernel would print out the string \"Workgroup index: 1\\n\" when run with a single workgroup (where \"\\n\" means a newline).","category":"page"},{"location":"printing/","page":"Printing","title":"Printing","text":"Any number of variables may be passed to @rocprintf, as long as those variables have a printf-compatible implementation in Printf.@printf. Calls to @rocprintf are blocking, and will not return control to the kernel until the string has been formatted and sent to the OS runtime for printing (the same as for calls to Printf.@printf).","category":"page"},{"location":"printing/","page":"Printing","title":"Printing","text":"While @rocprintf is printed once per workgroup by default, it's possible to print once per lane, once per wavefront, or once per grid by specifying an execution mode as the first argument:","category":"page"},{"location":"printing/","page":"Printing","title":"Printing","text":"# Once per lane\nkernel(x) = @rocprintf :lane \"My index is: %d\\n\" workitemIdx().x\n\n# Once per wavefront\nkernel(x) = @rocprintf :wave \"My index is: %d\\n\" workitemIdx().x\n\n# Once per workgroup\nkernel(x) = @rocprintf :group \"My index is: %d\\n\" workitemIdx().x\n# OR (:group is the default)\nkernel(x) = @rocprintf \"My index is: %d\\n\" workitemIdx().x\n\n# Once total\nkernel(x) = @rocprintf :grid \"My index is: %d\\n\" workitemIdx().x","category":"page"},{"location":"printing/","page":"Printing","title":"Printing","text":"Executing those kernels with 256 workitems split evenly between 2 workgroups would print out:","category":"page"},{"location":"printing/","page":"Printing","title":"Printing","text":"# :lane\nMy index is 1\nMy index is 2\n...\nMy index is 127\nMy index is 128\nMy index is 1\nMy index is 2\n...\nMy index is 127\nMy index is 128\n\n# :wave\nMy index is 1\nMy index is 65\nMy index is 1\nMy index is 65\n\n# :group\nMy index is 1\nMy index is 1\n\n# :grid\nMy index is 1","category":"page"},{"location":"printing/#Differences-to-@cuprintf","page":"Printing","title":"Differences to @cuprintf","text":"","category":"section"},{"location":"printing/","page":"Printing","title":"Printing","text":"Similar to CUDA's @cuprintf, @rocprintf is a printf-compatible macro which takes a format string and arguments, and commands the host CPU to display it as formatted text. However, in contrast to @cuprintf, we use AMDGPU's hostcall and Julia's Printf stdlib to implement this. This means that anything that Printf can print, so can @rocprintf (assuming such an object can be represented on the GPU). The macro is also handled as a regular hostcall, which means that argument types are checked at compile time (although currently, any errors while printing will be detected on the host, and will terminate the kernel).","category":"page"},{"location":"#Programming-AMD-GPUs-with-Julia","page":"Home","title":"Programming AMD GPUs with Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"tip: Tip\nThis documentation assumes that you are familiar with the main concepts of GPU programming and mostly describes the specifics of running Julia code on AMD GPUs. For a much more gentle introduction to GPGPU with Julia consult the well-written CUDA.jl documentation.","category":"page"},{"location":"#The-ROCm-stack","page":"Home","title":"The ROCm stack","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ROCm (short for Radeon Open Compute platforM) is AMD's open-source GPU computing platform, supported by most modern AMD GPUs (detailed hardware support) and some AMD APUs. ROCm works solely on Linux and no plans to support either Windows or macOS have been announced by AMD.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A necessary prerequisite to use this Julia package is to have a working ROCm stack installed. A quick way to verify this is to check the output of rocminfo. For more information, consult the official installation guide. Even though the only platforms officially supported by AMD are certain versions of Ubuntu, CentOS, RHEL, and SLES [1], there are options to install ROCm on other Linux distributions, including:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Arch Linux - See the rocm-arch repository or the slightly older PKGBUILDs in the AUR.\nGentoo - Check Portage for the rocr-runtime package and justxi's rocm repo for unofficial ROCm package ebuilds.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[1]: https://github.com/RadeonOpenCompute/ROCm/wiki#supported-operating-systems","category":"page"},{"location":"#The-Julia-AMDGPU-stack","page":"Home","title":"The Julia AMDGPU stack","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Julia support for programming AMD GPUs is currently provided by the AMDGPU.jl package. This package contains everything necessary to program for AMD GPUs in Julia, including:","category":"page"},{"location":"","page":"Home","title":"Home","text":"An interface for working with the HSA runtime API, necessary for launching compiled kernels and controlling the GPU.\nAn interface for compiling and running kernels written in Julia through LLVM's AMDGPU backend.\nAn array type implementing the GPUArrays.jl interface, providing high-level array operations.","category":"page"},{"location":"#Required-Software","page":"Home","title":"Required Software","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ROCT (JLL available)\nROCR (JLL available)\nROCm-Device-Libs (JLL available)\nHIP (JLL available)\nRecent Linux kernel with AMDGPU and HSA enabled (Cannot be provided as a JLL)\nld.lld binary provided by system LLVM (No JLL yet)","category":"page"},{"location":"#Optional-Packages","page":"Home","title":"Optional Packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"rocBLAS for BLAS support (JLL available)\nrocFFT for FFT support (No JLL yet)\nrocRAND for RNG support (JLL available)\nMIOpen for DNN support (JLL available on Julia 1.9)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Other ROCm packages are currently unused by AMDGPU.","category":"page"},{"location":"#JLL-usage","page":"Home","title":"JLL usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"By default, AMDGPU provides and uses JLL packages for core libraries, so as long as ld.lld is available, you should be all set for most basic functionality. For example, Julia 1.9 provides ROCm 5.2.3 libraries.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If this does not work for you, or if you have a full ROCm installation available on your system (common for HPC/supercomputer users), you can set the JULIA_AMDGPU_DISABLE_ARTIFACTS environment variable to \"1\" to disable usage of JLL artifacts:","category":"page"},{"location":"","page":"Home","title":"Home","text":"JULIA_AMDGPU_DISABLE_ARTIFACTS=1 julia --project=.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that currently ROCm-Device-Libs are always provided by AMDGPU to ensure compatibility with Julia's version of LLVM; please file an issue if this is problematic on your system.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: LLVM compatibility\nFor proper support, Julia's LLVM version should match ROCm LLVM's version. For example, Julia 1.9 relies on LLVM 14, so the matching ROCm version is 5.2.x.","category":"page"},{"location":"#Extra-Setup-Details","page":"Home","title":"Extra Setup Details","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Currently, the requirements to get everything working properly is a bit poorly documented in the upstream docs for any distro other than Ubuntu. So here is a list of requirements I've found through the process of making this work:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Make sure /dev/kfd has a group other than root that you can add your user to.   I recommend adding your user to the video group, and setting the   ownership of /dev/kfd to root:video with 660 permissions.\nThese libraries should be in the standard library locations, or in your LD_LIBRARY_PATH:\nlibhsakmt.so\nlibhsa-runtime64.so.1\nlibamdhip64.so\nAnd ld.lld should be in your PATH.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In terms of Linux kernel versions, just pick the newest one you can. If building your own kernel, make sure all the regular AMDGPU and HSA options are enabled.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Once all of this is setup properly, you should be able to do using AMDGPU successfully. See the Quickstart documentation for an introduction to using AMDGPU.jl.","category":"page"},{"location":"#Navi-2-(GFX103x)-support","page":"Home","title":"Navi 2 (GFX103x) support","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ROCm stack officially supports only GFX1030 (6900XT). However, the ISA between GFX103x devices is nearly identical (if not identical).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Therefore, if you have any other GFX103x device, you can override your gfx version with HSA_OVERRIDE_GFX_VERSION=10.3.0 env variable before launching Julia and be able to use your device:","category":"page"},{"location":"","page":"Home","title":"Home","text":"HSA_OVERRIDE_GFX_VERSION=10.3.0 julia --project=.","category":"page"},{"location":"kernel_deps/#Kernel-Dependencies","page":"Kernel Dependencies","title":"Kernel Dependencies","text":"","category":"section"},{"location":"kernel_deps/","page":"Kernel Dependencies","title":"Kernel Dependencies","text":"Unlike CUDA, ROCm does not have blocking queues; instead, all kernels placed on a queue will usually be processed and scheduled immediately. There is one exception: barrier packets may be placed on the queue to block the GPU's queue packet processor from proceeding until a given set of kernels has completed. These barriers come in two flavors: barrier_and! and barrier_or!. These functions can be called on a queue with a given set of kernel signals (those returned from @roc) to wait for all kernels or any one kernel to complete, respectively.","category":"page"},{"location":"kernel_deps/","page":"Kernel Dependencies","title":"Kernel Dependencies","text":"Generally, the barrier_and! call should be the most useful tool for most users, since many codes require synchronization of all \"threads of execution\" at the end of one step before moving onto the next step. For example, the following code may look innocuous, but in fact the kernels might \"race\" and return unexpected results:","category":"page"},{"location":"kernel_deps/","page":"Kernel Dependencies","title":"Kernel Dependencies","text":"function kernel(A)\n    A[1] += 1.0\n    nothing\nend\n\nRA = ROCArray(zeros(Float64, 1))\n@roc kernel(RA)\n@roc kernel(RA)\n@show Array(RA)[1] # could be 1.0 or 2.0","category":"page"},{"location":"kernel_deps/","page":"Kernel Dependencies","title":"Kernel Dependencies","text":"To fix this example, we use a barrier_and! call to ensure proper ordering of execution:","category":"page"},{"location":"kernel_deps/","page":"Kernel Dependencies","title":"Kernel Dependencies","text":"RA = ROCArray(zeros(Float64, 1))\ns1 = @roc kernel(RA)\nbarrier_and!([s1])\ns2 = @roc kernel(RA)\nwait(s2)\n@show Array(RA)[1] # will always be 2.0","category":"page"},{"location":"kernel_deps/","page":"Kernel Dependencies","title":"Kernel Dependencies","text":"While likely less useful for most, barrier_or! can be useful in situations where any one of many \"input\" kernels can satisfy a condition necessary to allow later kernels to execute properly:","category":"page"},{"location":"kernel_deps/","page":"Kernel Dependencies","title":"Kernel Dependencies","text":"function kernel1(A, i)\n    A[1] = i\n    nothing\nend\nfunction kernel2(A, i)\n    A[2] = i/A[1]\nend\n\nRA = ROCArray(zeros(Float64, 2))\ns1 = @roc kernel1(RA, 1.0)\ns2 = @roc kernel1(RA, 2.0)\nbarrier_or!([s1,s2])\ns3 = @roc kernel2(RA, 3.0)\nwait(s3)\n@show Array(RA)[1] # will either be 3.0 or 1.5, but will never throw due to divide-by-zero","category":"page"},{"location":"kernel_deps/","page":"Kernel Dependencies","title":"Kernel Dependencies","text":"warning: Warning\nBecause of how barrier OR packets work, you can't use queue hardware to do a wait-any on more than 5 signals at a time. If more than 5 signals are specified, then the signals are split into sets of 5, and the total barrier won't be fulfilled until, for each set, one of the signals is satisfied.Contributions are welcome to workaround this issue, which will probably need to implemented in software either on the CPU or GPU side.","category":"page"},{"location":"exceptions/#Kernel-thrown-Exceptions","page":"Exceptions","title":"Kernel-thrown Exceptions","text":"","category":"section"},{"location":"exceptions/","page":"Exceptions","title":"Exceptions","text":"Just like regular CPU-executed Julia functions, GPU kernels can throw exceptions! For example, the following kernel will throw a KernelException:","category":"page"},{"location":"exceptions/","page":"Exceptions","title":"Exceptions","text":"function throwkernel(A)\n    A[0] = 1\nend\nRA = ROCArray(zeros(Int,1))\nwait(@roc throwkernel(RA))","category":"page"},{"location":"exceptions/","page":"Exceptions","title":"Exceptions","text":"Kernels that hit an exception will write some exception information into a pre-allocated list for the CPU to inspect. Once complete, the wavefront throwing the exception will stop itself, but other wavefronts will continue executing (possibly throwing their own exceptions, or not).","category":"page"},{"location":"exceptions/","page":"Exceptions","title":"Exceptions","text":"Kernel-thrown exceptions are thrown on the CPU in the call to wait(event), where event is the returned value of @roc calls. When the kernel signals that it's completed, the wait function will check if an exception flag has been set, and if it has, will collect all of the relevant exception information that the kernels set up. Unlike CPU execution, GPU kernel exceptions aren't very user-customizable and pretty (for now!). They don't call Base.show, but instead pass the LLVM function name of their exception handler (details in GPUCompiler, src/irgen.jl). Therefore, the exact error that occured might be a bit hard to figure out.","category":"page"},{"location":"exceptions/","page":"Exceptions","title":"Exceptions","text":"If exception checking turns out to be too expensive for your needs, you can disable those checks by passing the kwarg check_exceptions=false to the wait call, which will skip any error checking (although it will still wait for the kernel to signal completion).","category":"page"}]
}
