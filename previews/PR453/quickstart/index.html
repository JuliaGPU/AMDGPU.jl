<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Quick Start · AMDGPU.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="AMDGPU.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">AMDGPU.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Quick Start</a><ul class="internal"><li><a class="tocitem" href="#Installation"><span>Installation</span></a></li><li><a class="tocitem" href="#Running-a-simple-kernel"><span>Running a simple kernel</span></a></li><li><a class="tocitem" href="#Naming-conventions"><span>Naming conventions</span></a></li></ul></li><li><a class="tocitem" href="../devices/">Devices</a></li><li><a class="tocitem" href="../streams/">Streams</a></li><li><a class="tocitem" href="../kernel_launch/">Kernel Launch</a></li><li><a class="tocitem" href="../exceptions/">Exceptions</a></li><li><a class="tocitem" href="../profiling/">Profiling</a></li><li><a class="tocitem" href="../memory/">Memory</a></li><li><a class="tocitem" href="../hostcall/">Host-Call</a></li><li><span class="tocitem">Intrinsics</span><ul><li><a class="tocitem" href="../execution_control/">Execution Control</a></li><li><a class="tocitem" href="../wavefront_ops/">Wavefront Operations</a></li></ul></li><li><a class="tocitem" href="../printing/">Printing</a></li><li><a class="tocitem" href="../logging/">Logging</a></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Quick Start</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Quick Start</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaGPU/AMDGPU.jl/blob/master/docs/src/quickstart.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Quick-Start"><a class="docs-heading-anchor" href="#Quick-Start">Quick Start</a><a id="Quick-Start-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-Start" title="Permalink"></a></h1><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>See <a href="../#JLL-usage">JLL usage</a> for info about ROCm stack installation. Simply add the AMDGPU.jl package to your Julia environment:</p><pre><code class="language-julia hljs">using Pkg
Pkg.add(&quot;AMDGPU&quot;)</code></pre><p>You can then load the <code>AMDGPU</code> package and run the unit tests:</p><pre><code class="language-julia hljs">using AMDGPU
using Pkg
Pkg.test(&quot;AMDGPU&quot;)</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If you get an error message along the lines of <code>GLIB_CXX_... not found</code>, it&#39;s possible that the C++ runtime used to build the ROCm stack and the one used by Julia are different. If you built the ROCm stack yourself this is very likely the case since Julia normally ships with its own C++ runtime. For more information, check out this <a href="https://github.com/JuliaLang/julia/issues/34276">GitHub issue</a>.</p><p>A quick fix is to use the <code>LD_PRELOAD</code> environment variable to make Julia use the system C++ runtime library, for example:</p><pre><code class="language-sh hljs">LD_PRELOAD=/usr/lib/libstdc++.so julia</code></pre><p>Alternatively, you can build Julia from source as described <a href="https://github.com/JuliaLang/julia/blob/master/doc/build/build.md">here</a>.</p><p>You can quickly debug this issue by starting Julia and trying to load a ROCm library:</p><pre><code class="language-julia hljs">using Libdl
Libdl.dlopen(&quot;/opt/rocm/hsa/lib/libhsa-runtime64.so.1&quot;)</code></pre></div></div><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If during the build process you get an error message along the lines of <code>hipErrorNoBinaryForGpu: Coudn&#39;t find binary for current devices!</code> and you already have ROCm installed locally then you should set the environment variable <code>JULIA_AMDGPU_DISABLE_ARTIFACTS=1</code> and reload AMDGPU.jl.</p></div></div><h2 id="Running-a-simple-kernel"><a class="docs-heading-anchor" href="#Running-a-simple-kernel">Running a simple kernel</a><a id="Running-a-simple-kernel-1"></a><a class="docs-heading-anchor-permalink" href="#Running-a-simple-kernel" title="Permalink"></a></h2><p>As a simple test, we will try to add two random vectors and make sure that the results from the CPU and the GPU are indeed the same.</p><p>We can start by first performing this simple calculation on the CPU:</p><pre><code class="language-julia hljs">N = 1024
a = rand(Float64, N)
b = rand(Float64, N)
c_cpu = a + b</code></pre><p>To do the same computation on the GPU, we first need to copy the two input arrays <code>a</code> and <code>b</code> to the device. Toward that end, we will use the <code>ROCArray</code> type to represent our GPU arrays. We can create the two arrays by passing the host data to the constructor as follows:</p><pre><code class="language-julia hljs">using AMDGPU
a_d = ROCArray(a)
b_d = ROCArray(b)</code></pre><p>We need to create one additional array <code>c_d</code> to store the results:</p><pre><code class="language-julia hljs">c_d = similar(a_d)</code></pre><p>In this example, the postfix <code>_d</code> distinguishes a device memory object from its host memory counterpart. This convention is completely arbitrary and you may name your device-side variables whatever you like; they are regular Julia variables.</p><p>Next, we will define the GPU kernel that does the actual computation:</p><pre><code class="language-julia hljs">function vadd!(c, a, b)
    i = workitemIdx().x + (workgroupIdx().x - 1) * workgroupDim().x
    c[i] = a[i] + b[i]
    return
end</code></pre><p>The index of a single workitem can be uniquely identified by its grid index (computed linearly as <code>(workgroupDim().x * (workgroupIdx().x - 1)) + workitemIdx().x</code> when only a single dimension is used).</p><p>The grid is the domain over which the <em>entire</em> kernel executes over. The grid will be split into multiple workgroups by hardware automatically, and the kernel does not complete until all workgroups complete.</p><p>Like OpenCL, AMDGPU has the concept of &quot;workitems&quot;, &quot;workgroups&quot;, and the &quot;grid&quot;. A workitem is a single thread of execution, capable of performing arithmentic operations. Workitems are grouped into &quot;wavefronts&quot; (&quot;warps&quot; in CUDA) which share the same compute unit, and execute the same instructions simulatenously. The workgroup is a logical unit of compute supported by hardware which comprises multiple wavefronts, which shares resources (specifically local memory) and can be efficiently synchronized. A workgroup may be executed by one or multiple hardware compute units, making it often the only dimension of importance for smaller kernel launches.</p><p>Notice how we explicitly specify that this function does not return a value by adding the <code>return</code> statement. This is necessary for all GPU kernels and we can enforce it by adding a <code>return</code>, <code>return nothing</code>, or even <code>nothing</code> at the end of the kernel. If this statement is omitted, Julia will attempt to return the value of the last evaluated expression, in this case a <code>Float64</code>, which will cause a compilation failure as kernels cannot return values.</p><p>The easiest way to launch a GPU kernel is with the <a href="@ref"><code>@roc</code></a> macro, specifying <code>groupsize</code> and <code>gridsize</code> to cover full array, and calling it like a regular function:</p><pre><code class="language-julia hljs">groupsize = 128
gridsize = cld(length(c_d), groupsize)
@roc gridsize=gridsize groupsize=groupsize vadd!(c_d, a_d, b_d)</code></pre><p>Keep in mind that kernel launches are asynchronous, meaning that you need to do some kind of synchronization before you use the result. For instance, you can call <code>AMDGPU.synchronize()</code>:</p><pre><code class="language-julia hljs">@roc groupsize=N vadd!(c_d, a_d, b_d)
AMDGPU.synchronize()</code></pre><p>Finally, we can make sure that the results match, by first copying the data to the host and then comparing it with the CPU results:</p><pre><code class="language-julia hljs">c = Array(c_d)

using Test
@test isapprox(c, c_cpu)</code></pre><h2 id="Naming-conventions"><a class="docs-heading-anchor" href="#Naming-conventions">Naming conventions</a><a id="Naming-conventions-1"></a><a class="docs-heading-anchor-permalink" href="#Naming-conventions" title="Permalink"></a></h2><p>Throughout this example we use terms like &quot;work group&quot; and &quot;work item&quot;. These terms are used by the Khronos consortium and their APIs including OpenCL and Vulkan, as well as the HSA foundation.</p><p>NVIDIA, on the other hand, uses some different terms in their CUDA API, which might be confusing to some users porting their kernels from CUDA to AMDGPU.</p><p>As a quick summary, here is a mapping of the most common terms:</p><table><tr><th style="text-align: center">AMDGPU</th><th style="text-align: center">CUDA</th></tr><tr><td style="text-align: center"><a href="../api/#AMDGPU.Device.workitemIdx"><code>workitemIdx</code></a></td><td style="text-align: center"><a href="@ref"><code>threadIdx</code></a></td></tr><tr><td style="text-align: center"><a href="../api/#AMDGPU.Device.workgroupIdx"><code>workgroupIdx</code></a></td><td style="text-align: center"><a href="@ref"><code>blockIdx</code></a></td></tr><tr><td style="text-align: center"><a href="../api/#AMDGPU.Device.workgroupDim"><code>workgroupDim</code></a></td><td style="text-align: center"><a href="@ref"><code>blockDim</code></a></td></tr><tr><td style="text-align: center"><a href="../api/#AMDGPU.Device.gridItemDim"><code>gridItemDim</code></a></td><td style="text-align: center">No equivalent</td></tr><tr><td style="text-align: center"><a href="../api/#AMDGPU.Device.gridGroupDim"><code>gridGroupDim</code></a></td><td style="text-align: center"><code>gridDim</code></td></tr><tr><td style="text-align: center"><code>groupsize</code></td><td style="text-align: center"><code>threads</code></td></tr><tr><td style="text-align: center"><code>gridsize</code></td><td style="text-align: center"><code>blocks</code></td></tr><tr><td style="text-align: center"><code>stream</code></td><td style="text-align: center"><code>stream</code></td></tr></table><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Since AMDGPU v0.5.0 <code>gridsize</code> represents the number of &quot;workgroups&quot; (or <code>blocks</code> in CUDA) and no longer &quot;workitems * workgroups&quot; (or <code>threads * blocks</code> in CUDA) as HIP is used for kernel launches instead of HSA.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../devices/">Devices »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Friday 28 July 2023 16:53">Friday 28 July 2023</span>. Using Julia version 1.9.3-DEV.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
